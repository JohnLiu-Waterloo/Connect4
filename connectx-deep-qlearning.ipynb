{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Install kaggle-environments"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-19T04:38:13.855491Z","iopub.status.busy":"2022-08-19T04:38:13.854775Z","iopub.status.idle":"2022-08-19T04:38:34.697489Z","shell.execute_reply":"2022-08-19T04:38:34.696270Z","shell.execute_reply.started":"2022-08-19T04:38:13.855186Z"},"trusted":true},"outputs":[],"source":["# 1. Enable Internet in the Kernel (Settings side pane)\n","\n","# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n","# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n","\n","# ConnectX environment was defined in v0.1.6\n","!pip install 'kaggle-environments>=0.1.6'"]},{"cell_type":"markdown","metadata":{},"source":["# Create ConnectX Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-08-19T04:38:34.702714Z","iopub.status.busy":"2022-08-19T04:38:34.702244Z","iopub.status.idle":"2022-08-19T04:38:34.972682Z","shell.execute_reply":"2022-08-19T04:38:34.971681Z","shell.execute_reply.started":"2022-08-19T04:38:34.702631Z"},"trusted":true},"outputs":[],"source":["from kaggle_environments import evaluate, make, utils\n","\n","env = make(\"connectx\", debug=True)\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T04:38:34.975155Z","iopub.status.busy":"2022-08-19T04:38:34.974529Z","iopub.status.idle":"2022-08-19T04:38:40.685655Z","shell.execute_reply":"2022-08-19T04:38:40.684903Z","shell.execute_reply.started":"2022-08-19T04:38:34.975000Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","import os # for creating directories"]},{"cell_type":"markdown","metadata":{},"source":["# Create an Agent\n","\n","To create the submission, an agent function should be fully encapsulated (no external dependencies).  \n","\n","When your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T04:38:40.687217Z","iopub.status.busy":"2022-08-19T04:38:40.686771Z","iopub.status.idle":"2022-08-19T04:38:40.701386Z","shell.execute_reply":"2022-08-19T04:38:40.699879Z","shell.execute_reply.started":"2022-08-19T04:38:40.687159Z"},"trusted":true},"outputs":[],"source":["# evaluate whether player has C pieces in a row on the board\n","def count_pos(board, n_col, player, C):\n","    result = 0\n","    n_row = int(len(board) / n_col)\n","    # i is row, j is column\n","\n","    # check rows\n","    for i in range(n_row):\n","        count = 0\n","        for j in range(n_col):\n","            if board[i*n_col + j] == player:\n","                count = count + 1\n","            else:\n","                count = 0\n","            if count == C:\n","                result += 1\n","    \n","    # check columns\n","    for j in range(n_col):\n","        count = 0\n","        for i in range(n_row):\n","            if board[i*n_col + j] == player:\n","                count = count + 1\n","            else:\n","                count = 0\n","            if count == C:\n","                result += 1\n","\n","    # check \\ columns\n","    for offset in range(2*n_col):\n","        count = 0\n","        for start in range(0, 2*n_col):\n","            i = start - n_col\n","            j = i + offset\n","            if i < 0 or i >= n_row or j < 0 or j >= n_col:\n","                continue\n","            if board[i*n_col + j] == player:\n","                count = count + 1\n","            else:\n","                count = 0\n","            if count == C:\n","                result += 1\n","\n","    # check / columns\n","    for offset in range(2*n_col):\n","        count = 0\n","        for start in range(0, 2*n_col):\n","            i = start - n_col\n","            j = offset - i\n","            if i < 0 or i >= n_row or j < 0 or j >= n_col:\n","                continue\n","            if board[i*n_col + j] == player:\n","                count = count + 1\n","            else:\n","                count = 0\n","            if count == C:\n","                result += 1\n","    \n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T04:39:17.845620Z","iopub.status.busy":"2022-08-19T04:39:17.845157Z","iopub.status.idle":"2022-08-19T04:39:17.850003Z","shell.execute_reply":"2022-08-19T04:39:17.849167Z","shell.execute_reply.started":"2022-08-19T04:39:17.845577Z"},"trusted":true},"outputs":[],"source":["def player_won(board, n_col, player):\n","    return count_pos(board, n_col, player, 4) >= 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T04:39:20.777775Z","iopub.status.busy":"2022-08-19T04:39:20.777242Z","iopub.status.idle":"2022-08-19T04:39:20.783994Z","shell.execute_reply":"2022-08-19T04:39:20.783024Z","shell.execute_reply.started":"2022-08-19T04:39:20.777727Z"},"trusted":true},"outputs":[],"source":["# update board state when *player places a piece in *pos column\n","def make_move(board, n_col, pos, player):\n","    new_board = board.copy()\n","    n_row = int(len(board) / n_col)\n","    for i in range(0, n_row):\n","        idx = n_col * (n_row-1-i) + pos\n","        if new_board[idx] == 0:\n","            new_board[idx] = player\n","            return new_board\n","    print(\"Error placing {} on {}\", pos, board)\n","    return board"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T05:03:30.933005Z","iopub.status.busy":"2022-08-19T05:03:30.932650Z","iopub.status.idle":"2022-08-19T05:03:30.960339Z","shell.execute_reply":"2022-08-19T05:03:30.959425Z","shell.execute_reply.started":"2022-08-19T05:03:30.932963Z"},"trusted":true},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=5000) # double-ended queue; acts like list, but elements can be added/removed from either end\n","        self.current_memory = deque(maxlen=5000)\n","        self.gamma = 0.95 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n","        self.epsilon = 1 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n","        self.epsilon_decay = 0.995 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n","        self.epsilon_min = 0.01 # minimum amount of random exploration permitted\n","        self.learning_rate = 0.001 # rate at which NN adjusts models parameters via SGD to reduce cost \n","        self.model = self._build_model() # private method\n","\n","    def _build_model(self):\n","        # neural net to approximate Q-value function:\n","        model = Sequential()\n","        model.add(Dense(42, input_dim=self.state_size, activation='relu')) # 1st hidden layer; states as input\n","        model.add(Dense(24, activation='relu')) # 2nd hidden layer\n","        model.add(Dense(24, activation='relu')) # 3rd hidden layer\n","        model.add(Dense(self.action_size, activation='linear')) # output layer = number of possible actions\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        return model\n","    \n","    def remember(self, state, action, reward, next_state, done, n_moves):\n","        # list of previous experiences, enabling re-training later\n","        self.current_memory.append((state, action, reward, next_state, done, n_moves))\n","    \n","    # method to determine the next action given the current state\n","    def act(self, state):\n","        n_col = self.action_size\n","        valid_moves = [col for col in range(n_col) if state[0][col] == 0]\n","\n","        # first apply human heuristic to speed up training process       \n","        # make the move if player can win the game, reward = 10\n","        for m in valid_moves:\n","            new_board = make_move(state[0], n_col, m, 1)\n","            if player_won(new_board, n_col, 1):\n","                return m, 10\n","\n","        # make the move if player can prevent opponent from winning the game, reward = 2\n","        for m in valid_moves:\n","            new_board = make_move(state[0], n_col, m, 2)\n","            if player_won(new_board, n_col, 2):\n","                return m, 2\n","\n","        # exploration: if acting randomly, take random action\n","        if np.random.rand() <= self.epsilon: # \n","            return random.choice(valid_moves), 0\n","\n","        # exploitation: if not acting randomly, predict reward value based on current state\n","        act_values = self.model.predict(state)\n","        predicted_action = int(np.argmax(act_values[0])) # pick the action that will give the highest reward (i.e., go left or right?)\n","        if predicted_action not in valid_moves:\n","            # train model to not predict this action\n","            self.remember(state, predicted_action, -20, state, True, -1)\n","            \n","            # choose new action based on the best out of the valid choices\n","            valid_filter = [0 if state[0][col] == 0 else -1000 for col in range(self.action_size)]\n","            predicted_action = int(np.argmax(act_values[0] + valid_filter))\n","\n","        return predicted_action, 0\n","\n","    # method that trains NN with experiences sampled from memory\n","    def replay(self, batch_size):\n","        minibatch = random.sample(self.memory, batch_size) # sample a minibatch from memory\n","        for state, action, reward, next_state, done, _ in minibatch: # extract data for each minibatch sample\n","            target = reward # if done (boolean whether game ended or not, i.e., whether final state or not), then target = reward\n","            if not done: # if not done, then predict future discounted reward\n","                target = (reward + self.gamma * # (target) = reward + (discount rate gamma) * \n","                          np.amax(self.model.predict(next_state)[0])) # (maximum target Q based on future action a')\n","            target_f = self.model.predict(state) # approximately map current state to future discounted reward\n","            target_f[0][action] = target\n","            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss btwn target_f and y_hat\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","    def replay_game(self, reward, total_moves):\n","        for state, action, state_reward, next_state, done, n_moves in self.current_memory: # extract data for each minibatch sample\n","            target_f = self.model.predict(state) # approximately map current state to future discounted reward\n","            target = reward if n_moves == -1 else reward * pow(self.gamma, total_moves - n_moves)\n","            target_f[0][action] = reward * pow(self.gamma, total_moves - n_moves)\n","            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss btwn target_f and y_hat\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","        \n","        # add to total memory and clear current memory\n","        self.memory.extend(self.current_memory)\n","        self.current_memory.clear()\n","        \n","    def load(self, name):\n","        self.model.load_weights(name)\n","\n","    def save(self, name):\n","        self.model.save_weights(name)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Debug/Train your Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T05:03:34.188845Z","iopub.status.busy":"2022-08-19T05:03:34.188055Z","iopub.status.idle":"2022-08-19T05:03:34.232543Z","shell.execute_reply":"2022-08-19T05:03:34.231694Z","shell.execute_reply.started":"2022-08-19T05:03:34.188779Z"},"trusted":true},"outputs":[],"source":["my_agent = DQNAgent(env.configuration.columns * env.configuration.rows, env.configuration.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T05:03:34.325359Z","iopub.status.busy":"2022-08-19T05:03:34.324968Z","iopub.status.idle":"2022-08-19T05:12:48.756589Z","shell.execute_reply":"2022-08-19T05:12:48.755672Z","shell.execute_reply.started":"2022-08-19T05:03:34.325293Z"},"trusted":true},"outputs":[],"source":["# Play as first position against random agent.\n","trainer = env.train([None, \"negamax\"])\n","debug = False\n","\n","batch_size = 32\n","\n","print(env.configuration)\n","total = 0\n","for i in range(0, 300):\n","    observation = trainer.reset()\n","    n_moves = 0\n","    # run a simulation with the current Q-learning agent, then train the model using the result\n","    while not env.done:\n","        prev_state = np.reshape(observation.board.copy(), [1, my_agent.state_size])        \n","        my_action, my_reward = my_agent.act(prev_state)\n","        if debug:\n","            print(\"My Action\", my_action, \"reward \", my_reward)\n","        \n","        observation, reward, done, info = trainer.step(my_action)\n","        if debug:\n","            env.render(mode=\"ipython\", width=200, height=180, header=False, controls=False)\n","        \n","        next_state = np.reshape(observation.board.copy(), [1, my_agent.state_size]) \n","        if done:\n","            my_reward = reward * 10\n","        n_moves += 1\n","        my_agent.remember(prev_state, my_action, my_reward, next_state, done, n_moves)\n","\n","    my_agent.replay_game(my_reward, n_moves)\n","    env.render()\n","    \n","    # metric for evaluating how good agent is performing\n","    total += reward\n","    if i % 10 == 9:\n","        print(\"Game \", i, \" Average reward: \", total / (i+1))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate your Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T05:13:02.341067Z","iopub.status.busy":"2022-08-19T05:13:02.340746Z","iopub.status.idle":"2022-08-19T05:13:02.652516Z","shell.execute_reply":"2022-08-19T05:13:02.651154Z","shell.execute_reply.started":"2022-08-19T05:13:02.341019Z"},"trusted":true},"outputs":[],"source":["def game_agent(observation, configuration):\n","    state = np.reshape(observation.board.copy(), [1, my_agent.state_size])        \n","    return my_agent.act(state)[0]\n","\n","# Selects random valid column\n","def agent_random(obs, config):\n","    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n","    return random.choice(valid_moves)\n","\n","# Agents play one game round\n","env.run([game_agent, agent_random])\n","\n","# Show the game\n","env.render(mode=\"ipython\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T05:13:02.656330Z","iopub.status.busy":"2022-08-19T05:13:02.656005Z","iopub.status.idle":"2022-08-19T05:14:26.429091Z","shell.execute_reply":"2022-08-19T05:14:26.427995Z","shell.execute_reply.started":"2022-08-19T05:13:02.656279Z"},"trusted":true},"outputs":[],"source":["def mean_reward(rewards):\n","    return sum(r[0] for r in rewards) / float(len(rewards))\n","\n","# Run multiple episodes to estimate its performance.\n","print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [game_agent, \"random\"], num_episodes=50)))\n","print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [game_agent, \"negamax\"], num_episodes=50)))"]},{"cell_type":"markdown","metadata":{},"source":["# Play your Agent\n","Click on any column to place a checker there (\"manually select action\")."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# \"None\" represents which agent you'll manually play as (first or second player).\n","# env.play([None, \"negamax\"], width=500, height=450)"]},{"cell_type":"markdown","metadata":{},"source":["# Write Submission File\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T05:53:05.203186Z","iopub.status.busy":"2022-08-19T05:53:05.202546Z","iopub.status.idle":"2022-08-19T05:53:05.223441Z","shell.execute_reply":"2022-08-19T05:53:05.222360Z","shell.execute_reply.started":"2022-08-19T05:53:05.203130Z"},"trusted":true},"outputs":[],"source":["my_agent.save(\"model_mc_v0.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-30T17:59:14.592632Z","iopub.status.busy":"2022-07-30T17:59:14.592345Z","iopub.status.idle":"2022-07-30T17:59:14.600931Z","shell.execute_reply":"2022-07-30T17:59:14.600139Z","shell.execute_reply.started":"2022-07-30T17:59:14.592593Z"},"trusted":true},"outputs":[],"source":["import inspect\n","import os\n","\n","def write_agent_to_file(function, file):\n","    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n","        f.write(inspect.getsource(function))\n","        print(function, \"written to\", file)\n","\n","write_agent_to_file(game_agent, \"submission.py\")"]},{"cell_type":"markdown","metadata":{},"source":["# Validate Submission\n","Play your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n","\n","Why validate? This roughly verifies that your submission is fully encapsulated and can be run remotely."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-30T17:59:37.858462Z","iopub.status.busy":"2022-07-30T17:59:37.858138Z","iopub.status.idle":"2022-07-30T17:59:37.888687Z","shell.execute_reply":"2022-07-30T17:59:37.887176Z","shell.execute_reply.started":"2022-07-30T17:59:37.858422Z"},"trusted":true},"outputs":[],"source":["# Note: Stdout replacement is a temporary workaround.\n","#import sys\n","#out = sys.stdout\n","#submission = utils.read_file(\"/kaggle/working/submission.py\")\n","#agent = utils.get_last_callable(submission)\n","#sys.stdout = out\n","\n","#env = make(\"connectx\", debug=True)\n","#env.run([agent, agent])\n","#print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")"]},{"cell_type":"markdown","metadata":{},"source":["# Submit to Competition\n","\n","1. Commit this kernel.\n","2. View the commited version.\n","3. Go to \"Data\" section and find submission.py file.\n","4. Click \"Submit to Competition\"\n","5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
